{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f4b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba97bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01e6d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a6db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de25887",
   "metadata": {},
   "source": [
    "The code below is a simple way of encoding strings. Other uses of token libraries are SentencePiece (subword tokenizer, used by Google) and tiktoken (used by ChatGPT, has 50257 tokens, way more than the 65 characters that we have)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0509b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 47, 52, 45, 39, 54, 53, 56, 43]\n",
      "singapore\n"
     ]
    }
   ],
   "source": [
    "#this is a mapping from characters to integers\n",
    "# and back, for easy encoding and decoding\n",
    "\n",
    "# chars = !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "\n",
    "stoi = {c: i for i, c in enumerate(chars)} #means strings to integers\n",
    "itos = {i: c for i, c in enumerate(chars)} #means integers to strings\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('singapore'))\n",
    "print(decode(encode('singapore'))) #Google uses SentencePiece, GPT uses tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39015c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) #The first 1000 characters encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabd29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation set\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb94ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4144229",
   "metadata": {},
   "source": [
    "When we put [18, 47, 56, 57, 58,  1, 15, 47, 58] into a transformer, there will be 8 examples: In the context of 18, 47 comes next. When 18 & 47, 56 comes next. So on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e77242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is 47.\n",
      "When input is tensor([18, 47]), the target is 56.\n",
      "When input is tensor([18, 47, 56]), the target is 57.\n",
      "When input is tensor([18, 47, 56, 57]), the target is 58.\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is 1.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is 15.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47.\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58.\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context}, the target is {target}.') #This are the eight examples that come for the first 9 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c8fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.int64\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8]) torch.int64\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "When input is [24], the target is 43.\n",
      "When input is [24, 43], the target is 58.\n",
      "When input is [24, 43, 58], the target is 5.\n",
      "When input is [24, 43, 58, 5], the target is 57.\n",
      "When input is [24, 43, 58, 5, 57], the target is 1.\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39.\n",
      "When input is [44], the target is 53.\n",
      "When input is [44, 53], the target is 56.\n",
      "When input is [44, 53, 56], the target is 1.\n",
      "When input is [44, 53, 56, 1], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58], the target is 46.\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52], the target is 58.\n",
      "When input is [52, 58], the target is 1.\n",
      "When input is [52, 58, 1], the target is 58.\n",
      "When input is [52, 58, 1, 58], the target is 46.\n",
      "When input is [52, 58, 1, 58, 46], the target is 39.\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46.\n",
      "When input is [25], the target is 17.\n",
      "When input is [25, 17], the target is 27.\n",
      "When input is [25, 17, 27], the target is 10.\n",
      "When input is [25, 17, 27, 10], the target is 0.\n",
      "When input is [25, 17, 27, 10, 0], the target is 21.\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many independent sequences will we process in parallel?\n",
    "block_size = 8 #what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #generate batch_size number of random offsets, between 0 and len(data) - block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #first block_size characters starting at each offset\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #next block_size characters starting at each offset\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, xb.dtype) #4 sequences of 8 characters each\n",
    "print(xb) #the first 4 sequences of 8 characters\n",
    "print(yb.shape, yb.dtype) #4 sequences of 8 characters each\n",
    "print(yb) #the next 4 sequences of 8 characters each, shifted by one character\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1] #the first t+1 characters of the b-th sequence\n",
    "        target = yb[b, t] #the t-th character of the b-th sequence\n",
    "        print(f'When input is {context.tolist()}, the target is {target}.') #This are the 4 examples that come for the first 8 characters of each of the 4 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18333fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23786804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #each toeken directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #65 x 65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx) #(B, T, C) Batch, Time, Channels\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #B is batch size, T is block size, C is vocab size\n",
    "            logits = logits.view(B*T, C) #stretch the array\n",
    "            targets = targets.view(B*T) #one dimensionalize the targets\n",
    "            loss = F.cross_entropy(logits, targets) #Negative Log Likelihood Loss, in PyTorch doc, they look for B x C x T instead. So the previous step will re-shape logits to that format\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is B by T array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the logits for the next token\n",
    "            logits, loss = self(idx, None)\n",
    "            #focus on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #append the sampled token to the input sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B , T+1)\n",
    "        return idx  \n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape) #4 x 8 x 65, 4 sequences of 8 characters each, each character has a probability distribution over the 65 characters\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros(1,1, dtype=torch.long) #start with the first character, which is the first character in the vocabulary\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) #generate 100 characters, starting with the first character in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be5833",
   "metadata": {},
   "source": [
    "Above creates an Embedding table, so following our sample xb, the first index '24' will pluck the 24th row in the table, second index '43' will pluck the 43rd. Logits is like the score for the next character in sequence, we are predicting for what comes next. Original model looks random as the model is not trained on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c24b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a PyTorch optimizer\n",
    "#AdamW is a variant of Adam that decouples weight decay from the optimization step\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c63a4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3132691383361816\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train') #get a batch of data\n",
    "    logits, loss = m(xb, yb) #forward pass\n",
    "    optimizer.zero_grad(set_to_none=True) #zero the gradients   \n",
    "    loss.backward() #backward pass\n",
    "    optimizer.step() #update the parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3a5a5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thaueerrpanat thathe, s\n",
      "WAnd Fithee.\n",
      "\n",
      "Nobys\n",
      "I ld mb, qun mod y wousa darketwave nghof IORitok has whodirate are atit G t hant m,\n",
      "HAn\n",
      "CELUSt y XENTha bu.\n",
      "Wh blinouket th thiviglecldewist, trveayokeanguror mepes ' wexfalle spprdswhyealaiate foulokiou:\n",
      "Beron, dopetuletougr ldr;\n",
      "S:\n",
      "I mby ongow:\n",
      "An icinfr\n",
      "Pasouserou' fenas'stigous.\n",
      "Touser m an rore, I m; ist arasound theithathis hthe'le is t V:\n",
      "W:\n",
      "Be th Myoua Counaksor s ct?\n",
      "\n",
      "Totris t eat;\n",
      "DUCAn doy ho shembeervon worse anthaus, moulit ardidond, t rk\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros(1, 1, dtype=torch.long), max_new_tokens=500)[0].tolist())) #generate 100 characters, starting with the first character in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088a4b6",
   "metadata": {},
   "source": [
    "Tokens are not talking to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bf7c7",
   "metadata": {},
   "source": [
    "##The mathematical trick in self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "885667f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x= torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "#How to get our tokens to talk to one another? Work backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6ca5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) #bow stands for bag of words, which is a way to represent the input sequence as a weighted sum of the input tokens\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ca8a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) \n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B,T,C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df2a06",
   "metadata": {},
   "source": [
    "It should be True, but I am not sure why it is False. I inspected the numbers and they looked the same...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0de674e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1678ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 3: using Softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #set the upper triangular part to -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # (B, T, T) @ (B, T, C) -> (B,T,C)\n",
    "torch.allclose(xbow2, xbow3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9a0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 4: self-attention\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# self attention Head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) #key projection\n",
    "query = nn.Linear(C, head_size, bias=False) #query projection\n",
    "value = nn.Linear(C, head_size, bias=False) #value projection\n",
    "k = key(x) #B, T, head_size\n",
    "q = query(x) #B, T, head_size\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #set the upper triangular part to -inf, nodes are not allowed to communicate with future\n",
    "wei = F.softmax(wei, dim=-1) #apply softmax to get probabilities, exponentially + normalize\n",
    "#in encoder blocks, the masking is removed, but in decoder blocks, the masking is applied\n",
    "v = value(x) #B, T, head_size\n",
    "\n",
    "out = wei @ v # (B, T, T) @ (B, T, C) -> (B,T,C)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717989b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]  #These are the weights, and not uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1a460",
   "metadata": {},
   "source": [
    "How can a vowel find consonants in the past, but also in a data-dependent way? The beauty of self-attention: every single token emits 2 vectors - a query and a key. Query vector is 'What am I looking for' and the Key vector is 'What do I contain?'. The main affinity between these two vectors is to dot product both keys and queries to produce a weight. The higher that dot product, the higher the weight, means it is very relatable.\n",
    "\n",
    "Self-attention: key, queries, values come from the same source\\\n",
    "Cross-attention: separate set of nodes, where we pull information and apply them into our original set of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142cf47",
   "metadata": {},
   "source": [
    "Batch normalization layer: ensure that each neuron is unit Gaussian distribution, 0 mean and 1 SD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95ddb542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchNormId:\n",
    "\n",
    "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) #mean over the batch dimension\n",
    "        xvar = x.var(1, keepdim=True, unbiased=False) #batch variance\n",
    "        xhat = (x - xmean)/ torch.sqrt(xvar + self.eps) # normalize the input to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta # scale and shift the normalized input\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = BatchNormId(100)\n",
    "x = torch.randn(32, 100) #32 samples, 100 features\n",
    "x = module(x)\n",
    "x.shape #32 samples, 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6985f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1476), tensor(0.8847))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() #mean and std of the first feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
